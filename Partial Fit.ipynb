{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing as mp\n",
    "from glob import glob\n",
    "from IPython.core.display import HTML\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glymur\n",
    "import cv2\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import itertools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import time\n",
    "import warnings\n",
    "# from jupyterthemes import jtplot\n",
    "import swifter\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage import data\n",
    "from skimage.filters import threshold_yen, threshold_triangle, threshold_otsu, threshold_li, sobel\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from skimage import exposure\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import label\n",
    "from skimage import color\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage.feature import hog\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "rescale_size = (150, 270)\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "num_cores = mp.cpu_count()\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "warnings.filterwarnings('ignore')\n",
    "# jtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_location = \"/export/data/utkarsh\"\n",
    "data_location = drive_location + '/data'\n",
    "P4data_location = drive_location + \"/P4data\"\n",
    "glob(P4data_location + \"/*.csv\")\n",
    "\n",
    "metadata = pd.read_csv(P4data_location + '/P4_catalog_v1.1_metadata.csv')\n",
    "tiles_coord = pd.read_csv(P4data_location + '/P4_catalog_v1.1_tile_coords_final.csv')\n",
    "fan = pd.read_csv(P4data_location + \"/P4_catalog_v1.1_L1C_cut_0.5_fan.csv\")\n",
    "blotch = pd.read_csv(P4data_location + \"/P4_catalog_v1.1_L1C_cut_0.5_blotch.csv\")\n",
    "\n",
    "item = tiles_coord.iloc[0].obsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (1 channel images)\n",
    "    Calculates hog features for each img\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, y=None, orientations=9,\n",
    "                 pixels_per_cell=(8, 8),\n",
    "                 cells_per_block=(3, 3), block_norm='L2-Hys'):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.block_norm = block_norm\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    " \n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block,\n",
    "                       block_norm=self.block_norm)\n",
    " \n",
    "        try: # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "\n",
    "\n",
    "hogify = HogTransformer(\n",
    "            pixels_per_cell=(15, 15),\n",
    "            cells_per_block=(3,3),\n",
    "            orientations=9,\n",
    "            block_norm='L2-Hys')\n",
    "\n",
    "scalify = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD =  SGDClassifier(max_iter=10, tol=1e-2, loss = \"hinge\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 10, verbose = 0, penalty=\"elasticnet\")\n",
    "\n",
    "\n",
    "MLP = MLPClassifier(random_state=seed, max_iter=200, warm_start = True)\n",
    "\n",
    "HOG_pipeline = Pipeline([\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(15, 15),\n",
    "        cells_per_block=(3,3),\n",
    "        orientations=20,\n",
    "        block_norm='L2-Hys')\n",
    "    ),\n",
    "    ('scalify', StandardScaler()),\n",
    "    ('classify', SGD)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read time: 0.05s\n",
      "Preprocessing time: 0.58s\n",
      "Train time: 0.81s\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "path_to_data = drive_location + f\"/data25A.p\"\n",
    "data = pd.read_pickle(path_to_data)\n",
    "print(f\"Read time: {round(time.time() - start_time, 2)}s\")\n",
    "\n",
    "X_train = data['img']\n",
    "y_train = data['label']\n",
    "\n",
    "t0 = time.time()\n",
    "X_train_processed = scalify.fit_transform(list(X_train))\n",
    "print(f\"Preprocessing time: {round(time.time() - t0, 2)}s\")\n",
    "\n",
    "t1 = time.time()\n",
    "clf = MLP.partial_fit(X_train_processed, y_train, classes=np.unique(y_train))\n",
    "# clf = SGD.partial_fit(X_train_processed, y_train, classes=np.unique(y_train))\n",
    "# clf = RandomForestClassifier(n_estimators=50, random_state = seed, max_features = 10 , warm_start = True)\n",
    "# clf = clf.fit(X_train_processed, y_train)\n",
    "print(f\"Train time: {round(time.time() - t1, 2)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_partial_fit(path_to_data, clf, to_precict, aslesha_predict, live_predict = False, n_tree = 0):\n",
    "    t = time.time()\n",
    "    data = pd.read_pickle(path_to_data)\n",
    "    print(f\"Read time: {round(time.time() - t, 2)}s\")\n",
    "\n",
    "    X_train = data['img']\n",
    "    y_train = data['label']\n",
    "\n",
    "    t0 = time.time()\n",
    "    X_train_processed = scalify.fit_transform(list(X_train))\n",
    "    print(f\"Preprocessing time: {round(time.time() - t0, 2)}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "#     updated_clf = clf.partial_fit(X_train_processed, y_train, classes=np.unique(y_train),\n",
    "#                                   sample_weight = compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    updated_clf = clf.partial_fit(X_train_processed, y_train, classes=np.unique(y_train))\n",
    "#     clf.set_params(n_estimators= n_tree + 50)\n",
    "#     updated_clf = clf.fit(X_train_processed, y_train)\n",
    "    print(f\"Train time: {round(time.time() - t1, 2)}s\")\n",
    "    \n",
    "    if live_predict:\n",
    "        y_pred = updated_clf.predict(to_predict)\n",
    "        print('X_test Accuracy score: ', accuracy_score(y_test, y_pred))\n",
    "        predictions = updated_clf.predict(aslesha_predict)\n",
    "        y_test0 = predicted_fans['label']\n",
    "        print('Prediction Percentage Overlap: ', accuracy_score(predictions, y_test0))\n",
    "    return updated_clf\n",
    "\n",
    "path_to_data = drive_location + f\"/test1025.p\"\n",
    "t2 = time.time()\n",
    "data = pd.read_pickle(path_to_data) #This is out test data\n",
    "print(f\"Read time: {round(time.time() - t2, 2)}s\")\n",
    "\n",
    "X_test = data['img']\n",
    "y_test = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "path_to_class = drive_location + \"/classify_false_negative_all_D.p\"\n",
    "predicted_fans = pd.read_pickle(path_to_class)\n",
    "# predicted_fans = predicted_fans.head(5000)\n",
    "print(f\"Read time: {round(time.time() - t, 2)}s\")\n",
    "to_predict = scalify.fit_transform(list(X_test))\n",
    "aslesha_predict = scalify.fit_transform(list(predicted_fans['img']))\n",
    "print(f\"Preprocessing time: {round(time.time() - t, 2)}s\")\n",
    "predicted_fans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trees = 50\n",
    "\n",
    "# path_to_data = drive_location + f\"/data100.p\"\n",
    "# clf = my_partial_fit(path_to_data, clf, to_predict, aslesha_predict, live_predict = True, n_tree = trees)\n",
    "# trees += 50\n",
    "# print(\"\")\n",
    "\n",
    "# path_to_data = drive_location + f\"/data100.p\"\n",
    "# clf = my_partial_fit(path_to_data, clf, to_predict, aslesha_predict, live_predict = True, n_tree = trees)\n",
    "# trees += 50\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trees = 50\n",
    "# D Skipped due to image reading issues. \n",
    "for letter in ['A', 'B', 'C', 'D', 'F', 'G']:\n",
    "    path_to_data = drive_location + f\"/data2500{letter}.p\"\n",
    "    clf = my_partial_fit(path_to_data, clf, to_predict, aslesha_predict, live_predict = True, n_tree = trees)\n",
    "    trees += 50\n",
    "    print(f\"Finished {letter}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = drive_location + f\"/test1025.p\"\n",
    "t2 = time.time()\n",
    "data = pd.read_pickle(path_to_data) #This is out test data\n",
    "print(f\"Read time: {round(time.time() - t2, 2)}s\")\n",
    "\n",
    "X_test = data['img']\n",
    "y_test = data['label']\n",
    "\n",
    "t1 = time.time()\n",
    "y_pred = clf.predict(scalify.fit_transform(list(X_test)))\n",
    "cmx = confusion_matrix(y_test, y_pred)\n",
    "data_predictions = pd.DataFrame(data=y_pred, index= X_test.index)\n",
    "print(f\"Predict time: {round(time.time() - t1, 2)}s\")\n",
    "print(f\"Total Runtime: {round(time.time()-start_time, 2)}s\\n\")\n",
    "print('X_test Accuracy score: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", cmx)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "predictions = clf.predict(aslesha_predict)\n",
    "predicted_fans['prediction'] = predictions\n",
    "pred = predicted_fans['prediction']\n",
    "predicted_fans.drop(labels=['prediction'], axis=1,inplace = True)\n",
    "predicted_fans.insert(2, 'prediction', pred)\n",
    "y_test0 = predicted_fans['label']\n",
    "cmx = confusion_matrix(y_test0, predictions)\n",
    "print(f\"Computing time: {round(time.time() - t, 2)}s\")\n",
    "print('Prediction Percentage Overlap: ', accuracy_score(predictions, y_test0))\n",
    "print(\"Confusion Matrix:\\n\", cmx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "group_names = ['We all agree these are fan',\n",
    "               'Aslesha and ground truth say fan \\n I say not fan',\n",
    "               'Aslesha and I say fan \\n Ground truth say not fan',\n",
    "               'Ground truth and I say not fans \\n Aslesha says fans']\n",
    "# group_names = [\"ground truth says blotch\",\n",
    "#                \"ground truth says blotch\",\n",
    "#                \"ground truth says blotch\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says fan\",\n",
    "#                \"ground truth says fan\",\n",
    "#                \"ground truth says fan\"]\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cmx.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cmx.flatten()/np.sum(cmx)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cmx, annot=labels, fmt='', cmap='Blues')\n",
    "plt.title(\"Percentage Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "#1 is notfan\n",
    "#0 is fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (10,7))\n",
    "# group_names = ['True Positive',\n",
    "#                'False Negative',\n",
    "#                'False Positive',\n",
    "#                'True Negative']\n",
    "# group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "#                 cmx.flatten()]\n",
    "# group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "#                      cmx.flatten()/np.sum(cmx)]\n",
    "# labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "#           zip(group_names,group_counts,group_percentages)]\n",
    "# labels = np.asarray(labels).reshape(2,2)\n",
    "# sns.heatmap(cmx, annot=labels, fmt='', cmap='Blues')\n",
    "# plt.title(\"Percentage Confusion Matrix (Detectron fan Predictions)\")\n",
    "# plt.xlabel(\"Predicted Label\")\n",
    "# plt.ylabel(\"True Label\")\n",
    "# plt.savefig('cmx2.pdf')\n",
    "# #1 is notfan\n",
    "# #0 is fan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(filename):\n",
    "    savename = data_location + \"/{filename}_RGB.NOMAP.JP2\".format(filename=filename)\n",
    "    if os.path.exists(savename):\n",
    "        print(\"{} exists\".format(savename))\n",
    "        return\n",
    "    components = filename.split(\"_\")\n",
    "    l = 100*int(int(components[1])/100)\n",
    "    h = 100*int(1+int(components[1])/100)-1\n",
    "    url=\"https://hirise-pds.lpl.arizona.edu/download/PDS/EXTRAS/RDR/ESP/ORB_{low:06d}_{high:06d}/{filename}/{filename}_RGB.NOMAP.JP2\".format(low=l,high=h,filename=filename)\n",
    "    print(url)\n",
    "    myfile = requests.get(url)\n",
    "    with open(savename, 'wb') as file:\n",
    "      file.write(myfile.content)\n",
    "      file.flush()\n",
    "      file.close()\n",
    "    \n",
    "def load_file(filename):\n",
    "    savename = data_location + \"/{filename}_RGB.NOMAP.JP2\".format(filename=filename)\n",
    "    if not os.path.exists(savename):\n",
    "        download_file(filename)\n",
    "    return glymur.Jp2k(savename)\n",
    "\n",
    "\n",
    "nx,ny = 840, 648\n",
    "\n",
    "def cv2_imshow(a, **kwargs):\n",
    "    a = a.clip(0, 255).astype('uint8')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return plt.imshow(a, **kwargs)\n",
    "\n",
    "def get_image(tiles_coord, jp, irow):\n",
    "    row = tiles_coord.iloc[irow]\n",
    "    sx = slice(int(row.x_hirise-nx//2),int(row.x_hirise+nx//2))\n",
    "    sy = slice(int(row.y_hirise-ny//2),int(row.y_hirise+ny//2))\n",
    "    im16 = np.copy(jp[sy,sx])\n",
    "    ratio = np.amax(im16) / 256\n",
    "    img8 = (im16 / ratio).astype('uint8')\n",
    "    return img8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only plotting when Aslesha and I agree on what is a fan but the ground truth says no. \n",
    "max_iters  = min(len(predicted_fans['img']), 200)\n",
    "# max_iters = len(predicted_fans['img'])\n",
    "results = pd.DataFrame({'tile_id': predicted_fans.tile_id, 'item_loc': predicted_fans.item_loc,\n",
    "                        'actual': predicted_fans['label'], \n",
    "                        'predictions': predictions}, index = predicted_fans['img'].index)\n",
    "\n",
    "results = results.sort_values(by=['tile_id'])\n",
    "results = results.tail(max_iters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "row = tiles_coord[tiles_coord.tile_id == results.iloc[0].tile_id].squeeze()\n",
    "irow = row.name\n",
    "jp = load_file(row.obsid)\n",
    "img = get_image(tiles_coord, jp, irow)\n",
    "ax.imshow(img)\n",
    "tile_id = results.loc[results.index[0]].tile_id\n",
    "\n",
    "for index, result in tqdm(results.iterrows(), total=results.shape[0]):\n",
    "#     if result.actual == \"fan\" or result.predictions == \"fan\":\n",
    "#     if result.predictions == \"notfan\" or result.actual == \"fan\":\n",
    "#         continue\n",
    "    \n",
    "    row = tiles_coord[tiles_coord.tile_id == result.tile_id].squeeze()\n",
    "    irow = row.name\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    \n",
    "    if not tile_id == result.tile_id:\n",
    "        fig, ax = plt.subplots(figsize=(12,12)) \n",
    "        ax.imshow(img)\n",
    "    \n",
    "    rect = Rectangle(result.item_loc[0], width = result.item_loc[1] , \n",
    "                     height = result.item_loc[2],\n",
    "                     fill=False, edgecolor='blue', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    fontsize = 12\n",
    "    h_offset = 0.32\n",
    "    ax.text(result.item_loc[0][0], \n",
    "            result.item_loc[0][1] + result.item_loc[2] + 8 + h_offset * fontsize, f\"{result.predictions}\", \n",
    "            fontsize= fontsize, weight='bold', color = \"blue\")\n",
    "\n",
    "    ax.text(result.item_loc[0][0], result.item_loc[0][1] - h_offset * fontsize, f\"{result.actual}\", \n",
    "            fontsize= fontsize, weight='bold', color = \"darkred\")\n",
    "    \n",
    "    tile_id = result.tile_id\n",
    "    ax.set_title(f'{tile_id}')\n",
    "    red_patch = mpatches.Patch(color='darkred', label='Ground truth')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Prediction')\n",
    "    ax.legend(handles=[red_patch, blue_patch], fancybox=True, framealpha=0.5, shadow=True, borderpad=1)\n",
    "    plt.savefig(f\"{tile_id}.pdf\")\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = predicted_fans['label']\n",
    "predicted_fans.drop(labels=['label'], axis=1,inplace = True)\n",
    "del predicted_fans['img']\n",
    "predicted_fans.insert(2, 'ground_truth', lab)\n",
    "t1 = time.time()\n",
    "predicted_fans.to_json(\"processed_classify_false_negative.json\")\n",
    "t2 = time.time()\n",
    "print(f\"Write Time: {t2-t1}\")\n",
    "predicted_fans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
