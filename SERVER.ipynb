{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# !{sys.executable} -m pip install --upgrade pip\n",
    "# !{sys.executable} -m pip install numpy\n",
    "# !{sys.executable} -m pip install scikit-image\n",
    "# !{sys.executable} -m pip install matplotlib\n",
    "# !{sys.executable} -m pip install scikit-learn\n",
    "# !{sys.executable} -m pip install h5py\n",
    "# !{sys.executable} -m pip install pandas\n",
    "# !{sys.executable} -m pip install tables\n",
    "# !{sys.executable} -m pip install requests\n",
    "# !{sys.executable} -m pip install Glymur\n",
    "# !{sys.executable} -m pip install tqdm\n",
    "# !{sys.executable} -m pip install opencv-python\n",
    "# !{sys.executable} -m pip install keras\n",
    "# !{sys.executable} -m pip install tensorflow\n",
    "# !{sys.executable} -m pip install seaborn\n",
    "# !{sys.executable} -m pip install ipyparallel\n",
    "# !{sys.executable} -m pip install pyarrow\n",
    "# !{sys.executable} -m pip install swifter\n",
    "# !{sys.executable} -m pip install pandarallel\n",
    "# !{sys.executable} -m pip install dask\n",
    "# !{sys.executable} -m pip install matplotlib-scalebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import multiprocessing as mp\n",
    "from glob import glob\n",
    "from IPython.core.display import HTML\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import glymur\n",
    "import cv2\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import random\n",
    "import itertools\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import time\n",
    "import warnings\n",
    "# from jupyterthemes import jtplot\n",
    "import swifter\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import ndimage as ndi\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "import skimage\n",
    "from skimage import data\n",
    "from skimage.filters import threshold_yen, threshold_triangle, threshold_otsu, threshold_li, sobel\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.color import label2rgb\n",
    "from skimage import exposure\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.measure import label\n",
    "from skimage import color\n",
    "from skimage.transform import resize, rescale\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "rescale_size = (300, 300)\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "num_cores = mp.cpu_count()\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('dark_background')\n",
    "# jtplot.style(theme=\"monokai\", context=\"notebook\", ticks=True, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_location = \"/export/data/utkarsh\"\n",
    "data_location = drive_location + '/data'\n",
    "P4data_location = drive_location + \"/P4data\"\n",
    "glob(P4data_location + \"/*.csv\")\n",
    "\n",
    "metadata = pd.read_csv(P4data_location + '/P4_catalog_v1.1_metadata.csv')\n",
    "tiles_coord = pd.read_csv(P4data_location + '/P4_catalog_v1.1_tile_coords_final.csv')\n",
    "fan = pd.read_csv(P4data_location + \"/P4_catalog_v1.1_L1C_cut_0.5_fan.csv\")\n",
    "blotch = pd.read_csv(P4data_location + \"/P4_catalog_v1.1_L1C_cut_0.5_blotch.csv\")\n",
    "\n",
    "print(len(tiles_coord))\n",
    "print(len(fan))\n",
    "print(len(blotch))\n",
    "\n",
    "item = tiles_coord.iloc[0].obsid\n",
    "print(tiles_coord.iloc[0])\n",
    "print(item)\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "print(tiles_coord)\n",
    "print(fan)\n",
    "pd.set_option('display.expand_frame_repr', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_file(filename):\n",
    "    savename = data_location + \"/{filename}_RGB.NOMAP.JP2\".format(filename=filename)\n",
    "    if os.path.exists(savename):\n",
    "        print(\"{} exists\".format(savename))\n",
    "        return\n",
    "    components = filename.split(\"_\")\n",
    "    l = 100*int(int(components[1])/100)\n",
    "    h = 100*int(1+int(components[1])/100)-1\n",
    "    url=\"https://hirise-pds.lpl.arizona.edu/download/PDS/EXTRAS/RDR/ESP/ORB_{low:06d}_{high:06d}/{filename}/{filename}_RGB.NOMAP.JP2\".format(low=l,high=h,filename=filename)\n",
    "    print(url)\n",
    "    myfile = requests.get(url)\n",
    "    with open(savename, 'wb') as file:\n",
    "      file.write(myfile.content)\n",
    "      file.flush()\n",
    "      file.close()\n",
    "    \n",
    "def load_file(filename):\n",
    "    savename = data_location + \"/{filename}_RGB.NOMAP.JP2\".format(filename=filename)\n",
    "    if not os.path.exists(savename):\n",
    "        download_file(filename)\n",
    "    return glymur.Jp2k(savename)\n",
    "\n",
    "\n",
    "nx,ny = 840, 648\n",
    "\n",
    "def cv2_imshow(a, **kwargs):\n",
    "    a = a.clip(0, 255).astype('uint8')\n",
    "    # cv2 stores colors as BGR; convert to RGB\n",
    "    if a.ndim == 3:\n",
    "        if a.shape[2] == 4:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "        else:\n",
    "            a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return plt.imshow(a, **kwargs)\n",
    "\n",
    "def fan_mask(fan, tile):\n",
    "    xc,yc = fan.image_x - (tile.x_hirise-nx//2),fan.image_y - (tile.y_hirise-ny//2)\n",
    "    \n",
    "    fan_s = fan.distance / (1+np.tan(np.deg2rad(fan.spread//2)))\n",
    "    fan_r = fan_s * np.tan(np.deg2rad(fan.spread//2))\n",
    "    circ_c_points = np.cos(np.deg2rad(np.arange(0,180,10)))\n",
    "    circ_s_points = np.sin(np.deg2rad(np.arange(0,180,10)))\n",
    "    xp = np.hstack([0,\n",
    "          fan_s,\n",
    "          fan_s+fan_r*circ_s_points,\n",
    "          fan_s,\n",
    "          0\n",
    "         ])\n",
    "    yp = np.hstack([0,\n",
    "          fan_r,\n",
    "          fan_r*circ_c_points,\n",
    "          -fan_r,\n",
    "          0\n",
    "         ])\n",
    "    rx,ry = np.cos(np.deg2rad(fan.angle)), np.sin(np.deg2rad(fan.angle))\n",
    "    rot = np.array([[rx,-ry],[ry,rx]])\n",
    "    xr,yr=np.dot(rot,np.vstack([xp,yp]))\n",
    "\n",
    "    return (xc+xr, yc+yr)\n",
    "\n",
    "def blotch_mask(blotch, tile):\n",
    "    xc,yc = blotch.image_x - (tile.x_hirise-nx//2),blotch.image_y - (tile.y_hirise-ny//2)\n",
    "\n",
    "    t = np.linspace(0, 2*np.pi, 22)\n",
    "    xp = blotch.radius_1 * np.cos(t)\n",
    "    yp = blotch.radius_2 * np.sin(t)\n",
    "\n",
    "    rx,ry = np.cos(np.deg2rad(blotch.angle)), np.sin(np.deg2rad(blotch.angle))\n",
    "    rot = np.array([[rx,-ry],[ry,rx]])\n",
    "    xr,yr=np.dot(rot,np.vstack([xp,yp]))\n",
    "\n",
    "    return (xc+xr, yc+yr)\n",
    "\n",
    "\n",
    "def get_image(tiles_coord, jp, irow):\n",
    "    row = tiles_coord.iloc[irow]\n",
    "    sx = slice(int(row.x_hirise-nx//2),int(row.x_hirise+nx//2))\n",
    "    sy = slice(int(row.y_hirise-ny//2),int(row.y_hirise+ny//2))\n",
    "    im16 = np.copy(jp[sy,sx])\n",
    "    ratio = np.amax(im16) / 256\n",
    "    img8 = (im16 / ratio).astype('uint8')\n",
    "    return img8\n",
    "\n",
    "def show_image(tiles_coord, fan_or_blotch, jp, irow, isfan=True):\n",
    "    \"\"\"\n",
    "    isfan: True for fan and False for blotch\n",
    "    \"\"\"\n",
    "    row = tiles_coord.loc[irow]\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "    cv2_imshow(img8)\n",
    "\n",
    "    if isfan:\n",
    "      myfans = fan_or_blotch[fan_or_blotch.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "      for ifan, fan in myfans.iterrows():\n",
    "            \n",
    "            x,y = fan_mask(fan, row)\n",
    "            x = np.where(x<0, 1, x) \n",
    "            y = np.where(y<0, 1, y) \n",
    "            x = np.where(x>nx, nx - 1, x) \n",
    "            y = np.where(y>ny, ny - 1, y) \n",
    "            plt.plot(x,y,alpha=1.0)\n",
    "            \n",
    "    \n",
    "#     else:\n",
    "#       myblotches = fan_or_blotch[fan_or_blotch.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "#       print(tiles_coord.loc[irow].tile_id)\n",
    "#       for iblotch, blotch in myblotches.iterrows():\n",
    "#           x,y = blotch_mask(blotch, row)\n",
    "#           plt.plot(x,y,alpha=1.0) # Removed since we do not need blotches, false will imply no fans shown\n",
    "\n",
    "    return img8 # This will return an array as well\n",
    "\n",
    "\n",
    "irow = 400\n",
    "# for irow in [1,50,200,400,600,800,1200,1500,2000,2500,3000,3500,4000]:\n",
    "row = tiles_coord.iloc[irow]\n",
    "jp = load_file(row.obsid)\n",
    "print(row.obsid, row.tile_id)\n",
    "plt.figure(figsize = (16,8))\n",
    "show_image(tiles_coord, fan, jp, irow, isfan = True)\n",
    "plt.title(f\"{row.tile_id} /w Marked Fans\")\n",
    "# plt.title(f\"{row.tile_id} /w Raw\")\n",
    "plt.xlabel(\"Pixels (nx)\")\n",
    "plt.ylabel(\"Pixels (ny)\")\n",
    "scalebar = ScaleBar(1, box_alpha = 0.3) # 1 pixel = 0.2 meter\n",
    "plt.gca().add_artist(scalebar)\n",
    "plt.savefig(\"fig2.pdf\")\n",
    "    \n",
    "# print(show_image(tiles_coord, fan, jp, 100, isfan = True)) # (NO LONGER) Gives and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_bbox(minx, miny, maxx, maxy, scale = 1.2):\n",
    "    width = abs(maxx - minx)\n",
    "    height = abs(maxy - miny)\n",
    "    new_width = width * scale\n",
    "    new_height = height * scale\n",
    "    new_minx = minx - (new_width - width)//2\n",
    "    new_maxx = maxx + (new_width - width)//2\n",
    "    new_miny = miny - (new_height - height)//2\n",
    "    new_maxy = maxy + (new_height - height)//2\n",
    "    \n",
    "    if new_minx < 1:\n",
    "        new_minx = 1\n",
    "    if new_maxx > (nx-1):\n",
    "        new_maxx = nx - 1\n",
    "    if new_miny < 1:\n",
    "        new_miny = 1\n",
    "    if new_maxy > (ny-1):\n",
    "        new_maxy = ny - 1\n",
    "        \n",
    "    new_minx = int(new_minx)\n",
    "    new_miny = int(new_miny)\n",
    "    new_maxx = int(new_maxx)\n",
    "    new_maxy = int(new_maxy)\n",
    "    return new_minx, new_miny, new_maxx, new_maxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fan_box_marking_id(tiles_coord, jp, irow):\n",
    "    row = tiles_coord.iloc[irow]\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    row = tiles_coord.loc[irow]\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "    marking_id_list = []\n",
    "\n",
    "    myfans = fan[fan.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "    for ifan, fan0 in myfans.iterrows():\n",
    "        marking_id_list.append(myfans.marking_id[ifan])     \n",
    "        \n",
    "    if len(marking_id_list) == 0:\n",
    "        return [None]\n",
    "    return marking_id_list\n",
    "\n",
    "def extract_fan_box(tiles_coord, jp, irow, loc_list):\n",
    "    row = tiles_coord.loc[irow]\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "    cropped = []\n",
    "    myfans = fan[fan.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "\n",
    "    for ifan, fan0 in myfans.iterrows():\n",
    "        x,y = fan_mask(fan0, row)\n",
    "        x = np.where(x<0, 1, x) \n",
    "        y = np.where(y<0, 1, y) \n",
    "        x = np.where(x>nx, nx - 1, x) \n",
    "        y = np.where(y>ny, ny - 1, y)\n",
    "        min_x = int(min(x))\n",
    "        max_x = int(max(x))\n",
    "        min_y = int(min(y))\n",
    "        max_y = int(max(y))\n",
    "#         min_x, min_y, max_x, max_y = expand_bbox(min_x, min_y, max_x, max_y, scale = 1.2)\n",
    "        cropped.append(img8[min_y:max_y, min_x:max_x])\n",
    "        width = abs(max_x - min_x)\n",
    "        height = abs(max_y - min_y)\n",
    "        loc_list.append([(min_x, min_y), width, height])\n",
    "\n",
    "    if cropped == []:\n",
    "        return [None]\n",
    "    else:\n",
    "        return cropped # List of img8 files of cropped fan images\n",
    "\n",
    "def intersection(arr1, arr2):\n",
    "    p1 = pd.DataFrame([r.flatten() for r in arr1]).drop_duplicates()\n",
    "    p2 = pd.DataFrame([r.flatten() for r in arr2]).drop_duplicates()\n",
    "    res = p1.merge(p2)\n",
    "    return res\n",
    "\n",
    "def random_square_edges(threshold_size = 25):\n",
    "    x1 = randint(0, nx - 1)\n",
    "    y1 = randint(0, ny - 1)\n",
    "    x2 = randint(0, nx - 1)\n",
    "    y2 = randint(0, ny - 1) \n",
    "    \n",
    "    if abs(y2-y1) < threshold_size:\n",
    "#         print(\"random_square_edges() FAILED BOX SIZE: Trying again\")\n",
    "        return random_square_edges(threshold_size)\n",
    "\n",
    "    if abs(x2-x1) < threshold_size:\n",
    "#         print(\"random_square_edges() FAILED BOX SIZE: Trying again\")\n",
    "        return random_square_edges(threshold_size)\n",
    "        \n",
    "    if x1>x2 and y1>y2:\n",
    "        x_low = x2\n",
    "        x_high = x1\n",
    "        y_low = y2\n",
    "        y_high = y1\n",
    "        \n",
    "    if x1>x2 and y2>y1:\n",
    "        x_low = x2\n",
    "        x_high = x1\n",
    "        y_low = y1\n",
    "        y_high = y2\n",
    "        \n",
    "    if x2>x1 and y1>y2:\n",
    "        x_low = x1\n",
    "        x_high = x2\n",
    "        y_low = y2\n",
    "        y_high = y1\n",
    "        \n",
    "    if x2>x1 and y2>y1:\n",
    "        x_low = x1\n",
    "        x_high = x2\n",
    "        y_low = y1\n",
    "        y_high = y2\n",
    "        \n",
    "    return x_low, x_high, y_low, y_high\n",
    "\n",
    "def square_generator(xmin, xmax, ymin, ymax):\n",
    "    xrun = np.arange(xmin, xmax, 1)\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    \n",
    "    for xval in xrun:\n",
    "        yline = np.arange(ymin, ymax, 1)\n",
    "        xline = np.linspace(xval, xval, len(yline))\n",
    "        x = np.append(x, xline)\n",
    "        y = np.append(y, yline)\n",
    "    return x,y\n",
    "\n",
    "def extract_random_box(tiles_coord, jp, irow, loc_list ,check_intersection = False):\n",
    "    \"\"\" This function extracts a random box which is not a fan but may be a blotch\n",
    "    \"\"\"\n",
    "    \n",
    "    row = tiles_coord.iloc[irow]\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    row = tiles_coord.loc[irow]\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "\n",
    "    myfans = fan[fan.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "    \n",
    "    fansx = np.array([])\n",
    "    fansy = np.array([])\n",
    "    \n",
    "    x_low, x_high, y_low, y_high = random_square_edges(threshold_size = 20)\n",
    "\n",
    "        \n",
    "    if check_intersection:\n",
    "        for ifan, fan0 in myfans.iterrows():\n",
    "            x,y = fan_mask(fan0, row)\n",
    "            x = np.where(x<0, 1, x) \n",
    "            y = np.where(y<0, 1, y) \n",
    "            x = np.where(x>nx, nx - 1, x) \n",
    "            y = np.where(y>ny, ny - 1, y)\n",
    "            xtemp, ytemp = square_generator(min(x), max(x), min(y), max(y))\n",
    "            xtemp, ytemp = np.round(xtemp), np.round(ytemp)\n",
    "            fansx = np.append(fansx, xtemp)\n",
    "            fansy = np.append(fansy, ytemp)\n",
    "    #         plt.plot(xtemp,ytemp, alpha = 0.3, color = \"g\")        \n",
    "        X,Y = square_generator(x_low, x_high, y_low, y_high)\n",
    "        arr1 = np.vstack((fansx,fansy)).T\n",
    "        arr2 = np.vstack((X,Y)).T\n",
    "        intersect = intersection(arr1, arr2)\n",
    "        if intersect.size != 0:\n",
    "#             print(\"extract_random_box() FAILED INTERSECTION: Trying again\")\n",
    "            return extract_random_box(tiles_coord, jp, irow, check_intersection = True) #If intersecting with fan masks, recurse. \n",
    "        \n",
    "        maxx, minx = int(max(X)), int(min(X))\n",
    "        maxy, miny = int(max(Y)), int(min(Y))\n",
    "#     cv2_imshow(img8)\n",
    "#     plt.plot(X,Y, alpha = 0.3, color = \"r\")\n",
    "    else:\n",
    "        maxx, minx = int(x_high), int(x_low)\n",
    "        maxy, miny = int(y_high), int(y_low)\n",
    "    \n",
    "    width = abs(maxx - minx)\n",
    "    height = abs(maxy - miny)\n",
    "    loc_list += [(minx, miny), width, height]\n",
    "    \n",
    "    return img8[miny:maxy, minx:maxx]\n",
    "\n",
    "def plot_bar(y, loc='left', relative=True):\n",
    "    width = 0.35\n",
    "    if loc == 'left':\n",
    "        n = -0.5\n",
    "    elif loc == 'right':\n",
    "        n = 0.5\n",
    " \n",
    "    # calculate counts per type and sort, to ensure their order\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    sorted_index = np.argsort(unique)\n",
    "    unique = unique[sorted_index]\n",
    " \n",
    "    if relative:\n",
    "        # plot as a percentage\n",
    "        counts = 100*counts[sorted_index]/len(y)\n",
    "        ylabel_text = '% count'\n",
    "    else:\n",
    "        # plot counts\n",
    "        counts = counts[sorted_index]\n",
    "        ylabel_text = 'count'\n",
    " \n",
    "    xtemp = np.arange(len(unique))\n",
    " \n",
    "    plt.bar(xtemp + n*width, counts, align='center', alpha=.7, width=width)\n",
    "    plt.xticks(xtemp, unique)\n",
    "    plt.xlabel('Label Type')\n",
    "    plt.ylabel(ylabel_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blotch = pd.read_csv(P4data_location + \"/P4_catalog_v1.1_L1C_cut_0.5_blotch.csv\")\n",
    "\n",
    "def extract_blotch_box_marking_id(tiles_coord, jp, irow):\n",
    "    row = tiles_coord.iloc[irow]\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "    marking_id_list = []\n",
    "\n",
    "    myblotches = blotch[blotch.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "    \n",
    "    for iblotch, blotch0 in myblotches.iterrows():\n",
    "        marking_id_list.append(myblotches.marking_id[iblotch])     \n",
    "        \n",
    "    if len(marking_id_list) == 0:\n",
    "        return [None]\n",
    "    return marking_id_list\n",
    "\n",
    "def extract_blotch_box(tiles_coord, jp, irow, loc_list):\n",
    "    row = tiles_coord.loc[irow]\n",
    "    img8 = get_image(tiles_coord, jp, irow)\n",
    "    cropped = []\n",
    "    myblotches = blotch[blotch.tile_id == tiles_coord.loc[irow].tile_id]\n",
    "\n",
    "    for iblotch, blotch0 in myblotches.iterrows():\n",
    "        x,y = blotch_mask(blotch0, row)\n",
    "        x = np.where(x<0, 1, x) \n",
    "        y = np.where(y<0, 1, y) \n",
    "        x = np.where(x>nx, nx - 1, x) \n",
    "        y = np.where(y>ny, ny - 1, y)\n",
    "        min_x = int(min(x))\n",
    "        max_x = int(max(x))\n",
    "        min_y = int(min(y))\n",
    "        max_y = int(max(y))\n",
    "#         min_x, min_y, max_x, max_y = expand_bbox(min_x, min_y, max_x, max_y, scale = 1.2)\n",
    "        cropped.append(img8[min_y:max_y, min_x:max_x])\n",
    "        width = abs(max_x - min_x)\n",
    "        height = abs(max_y - min_y)\n",
    "        loc_list.append([(min_x, min_y), width, height])\n",
    "\n",
    "    if cropped == []:\n",
    "        return [None]\n",
    "    else:\n",
    "        return cropped # List of img8 files of cropped fan images\n",
    "    \n",
    "# for i in tqdm([1,2,5,20,29,100,899, 1000, 1001, 1002, 1003, 1004, 1005]):\n",
    "#     irow = i\n",
    "#     row = tiles_coord.iloc[irow]\n",
    "#     jp = load_file(row.obsid)\n",
    "#     img = get_image(tiles_coord, jp, irow)\n",
    "#     marking_id_list = extract_blotch_box_marking_id(tiles_coord, jp, irow)\n",
    "#     loc_list = []\n",
    "#     blotch_boxes = extract_blotch_box(tiles_coord, jp, irow, loc_list)\n",
    "# #     print(loc_list)\n",
    "#     count = 0\n",
    "#     for image in blotch_boxes:\n",
    "#         if image is None:\n",
    "#             continue\n",
    "#         plt.figure()\n",
    "#         plt.title(marking_id_list[count])    \n",
    "#         count += 1\n",
    "#         plt.imshow(image)\n",
    "# #         cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(img0):\n",
    "    img = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n",
    "    img = resize(image = img, output_shape = rescale_size, anti_aliasing=True)\n",
    "#     p2, p98 = np.percentile(img, (5, 95))\n",
    "#     rescaled = exposure.rescale_intensity(img, in_range=(p2, p98))\n",
    "#     sigma = 1.5\n",
    "#     thresh = np.mean(rescaled) -  sigma * np.std(rescaled) \n",
    "#     img = img > threshold_otsu(img)\n",
    "#     img = closing(rescaled < thresh, square(1))\n",
    "#     img = ndi.binary_fill_holes(img)\n",
    "    return img\n",
    "\n",
    "# irow = 8\n",
    "# row = tiles_coord.iloc[irow]\n",
    "# jp = load_file(row.obsid)\n",
    "\n",
    "# plt.figure(figsize = (16,9))\n",
    "# loc_list = []\n",
    "# random_img = extract_fan_box(tiles_coord, jp, irow, loc_list)[0]\n",
    "# plt.imshow(process_image(random_img))\n",
    "# plt.figure()\n",
    "# plt.imshow(random_img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of CPU processors:\", num_cores)\n",
    "\n",
    "print(\"No. Tiles:\", len(tiles_coord))\n",
    "print(\"Total Sample Size:\", len(fan))\n",
    "\n",
    "\n",
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (1 channel images)\n",
    "    Calculates hog features for each img\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, y=None, orientations=9,\n",
    "                 pixels_per_cell=(8, 8),\n",
    "                 cells_per_block=(3, 3), block_norm='L2-Hys'):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.block_norm = block_norm\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    " \n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block,\n",
    "                       block_norm=self.block_norm)\n",
    " \n",
    "        try: # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        \n",
    "        \n",
    "hogify = HogTransformer(\n",
    "            pixels_per_cell=(15, 15),\n",
    "            cells_per_block=(3,3),\n",
    "            orientations=9,\n",
    "            block_norm='L2-Hys')\n",
    "\n",
    "scalify = StandardScaler()\n",
    "\n",
    "data = pd.DataFrame({'tile_id' : [], \"marking_id\": [], \"label\": [], \"item_loc\": [],\"img\": []})\n",
    "\n",
    "# data_size = 2500\n",
    "\n",
    "# skip_factor = len(tiles_coord) // data_size\n",
    "\n",
    "# # print(f\"Extracting a tile every {skip_factor} tiles\")\n",
    "\n",
    "# def parallel_extract(i, rescale_size, tiles_coord):\n",
    "#     data = pd.DataFrame({'tile_id' : [], \"marking_id\": [], \"label\": [], \"item_loc\": [],\"img\": []})\n",
    "#     irow = i  + 2500 + 2500 + 2500 + 2500 + 2500 + 2500 + 2500\n",
    "#     # A + B + C + D + E + F + G => H\n",
    "#     row = tiles_coord.iloc[irow]\n",
    "#     jp = load_file(row.obsid)\n",
    "    \n",
    "#     marking_id_list = extract_fan_box_marking_id(tiles_coord, jp, irow)\n",
    "#     marking_id_list_blotch = extract_blotch_box_marking_id(tiles_coord, jp, irow)\n",
    "#     fan_locations = []\n",
    "#     blotch_locations = []\n",
    "#     fan_boxes = extract_fan_box(tiles_coord, jp, irow, fan_locations)\n",
    "#     blotch_boxes = extract_blotch_box(tiles_coord, jp, irow, blotch_locations)\n",
    "    \n",
    "#     if marking_id_list[0] is None:\n",
    "#         marking_id_list.pop(0)\n",
    "        \n",
    "#     if marking_id_list_blotch[0] is None:\n",
    "#         marking_id_list_blotch.pop(0)\n",
    "    \n",
    "# #     assert len(marking_id_list) == len(fan_boxes)\n",
    "    \n",
    "#     for k in range(len(marking_id_list_blotch)):\n",
    "        \n",
    "#         blotch_box = process_image(blotch_boxes[k])\n",
    "#         blotch_box = hog(blotch_box,\n",
    "#                          orientations= 9,\n",
    "#                          pixels_per_cell= (8, 8),\n",
    "#                          cells_per_block= (3, 3),\n",
    "#                          block_norm = 'L2-Hys')\n",
    "        \n",
    "#         boxed_blotch = pd.DataFrame({'tile_id' : [tiles_coord.iloc[irow].tile_id],\n",
    "#                             \"marking_id\": [marking_id_list_blotch[k]],\n",
    "#                             \"label\": [\"notfan\"], \n",
    "#                             \"item_loc\": [blotch_locations[k]],\n",
    "#                             \"img\": [blotch_box]})\n",
    "#         data = pd.concat([data, boxed_blotch])\n",
    "    \n",
    "#     for j in range(len(marking_id_list)):\n",
    "        \n",
    "#         fan_box = process_image(fan_boxes[j])\n",
    "        \n",
    "#         fan_box = hog(fan_box,\n",
    "#                         orientations= 9,\n",
    "#                         pixels_per_cell= (8, 8),\n",
    "#                         cells_per_block= (3, 3),\n",
    "#                         block_norm = 'L2-Hys')\n",
    "        \n",
    "# #         random_box_loc = []\n",
    "# #         random_box = extract_random_box(tiles_coord, load_file(tiles_coord.iloc[irow].obsid), irow, random_box_loc)\n",
    "# #         random_box = process_image(random_box)\n",
    "        \n",
    "# #         random_box2_loc = []\n",
    "# #         random_box2 = extract_random_box(tiles_coord, load_file(tiles_coord.iloc[irow].obsid), irow, random_box2_loc)\n",
    "# #         random_box2 = process_image(random_box2)\n",
    "\n",
    "# #         random_box3_loc = []\n",
    "# #         random_box3 = extract_random_box(tiles_coord, load_file(tiles_coord.iloc[irow].obsid), irow, random_box3_loc)\n",
    "# #         random_box3 = cv2.cvtColor(random_box3, cv2.COLOR_BGR2GRAY)\n",
    "# #         random_box3 = resize(image = random_box3, output_shape = rescale_size, anti_aliasing=True)        \n",
    "        \n",
    "#         boxed_fan = pd.DataFrame({'tile_id' : [tiles_coord.iloc[irow].tile_id],\n",
    "#                             \"marking_id\": [marking_id_list[j]],\n",
    "#                             \"label\": [\"fan\"], \n",
    "#                             \"item_loc\": [fan_locations[j]],\n",
    "#                             \"img\": [fan_box]})\n",
    "#         data = pd.concat([data, boxed_fan])\n",
    "        \n",
    "# #         random = pd.DataFrame({'tile_id' : [tiles_coord.iloc[irow].tile_id],\n",
    "# #                             \"marking_id\": [\"R\" + marking_id_list[j][1:]],\n",
    "# #                             \"label\": [\"notfan\"], \n",
    "# #                             \"item_loc\": [random_box_loc],\n",
    "# #                             \"img\": [random_box]})\n",
    "# #         data = pd.concat([data, random])\n",
    "        \n",
    "        \n",
    "# #         random2 = pd.DataFrame({'tile_id' : [tiles_coord.iloc[irow].tile_id],\n",
    "# #                             \"marking_id\": [\"E\" + marking_id_list[j][1:]],\n",
    "# #                             \"label\": [\"notfan\"], \n",
    "# #                             \"item_loc\": [random_box2_loc],\n",
    "# #                             \"img\": [random_box2]})\n",
    "# #         data = pd.concat([data, random2])\n",
    "\n",
    "# #         random3 = pd.DataFrame({'tile_id' : [tiles_coord.iloc[irow].tile_id],\n",
    "# #                             \"marking_id\": [\"Q\" + marking_id_list[j][1:]],\n",
    "# #                             \"label\": [\"notfan\"], \n",
    "# #                             \"item_loc\": [random_box3_loc],\n",
    "# #                             \"img\": [random_box3]})\n",
    "# #         data = pd.concat([data, random3])\n",
    "#     return data\n",
    "\n",
    "# # with parallel_backend('threading', n_jobs=4):\n",
    "# current = Parallel(n_jobs = 50, verbose=8, prefer = 'threads')(delayed(parallel_extract)\\\n",
    "#                                                   (i, rescale_size, tiles_coord) for i in range(data_size))\n",
    "# all_data = pd.concat(current)\n",
    "# data = pd.concat([data, all_data])\n",
    "# data.reset_index(drop=True, inplace=True)\n",
    "# print(\"Current Sample Size:\" , data.shape[0])\n",
    "\n",
    "# t = time.time()\n",
    "# path_to_data = drive_location + f\"/data{data_size}H.p\"\n",
    "# # path_to_data = drive_location + f\"/test1025.p\"\n",
    "# data.to_pickle(path_to_data, protocol = 3)\n",
    "# print(f\"Write time: {round(time.time() - t, 2)}s\")\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_to_extract = 2000\n",
    "t = time.time()\n",
    "path_to_data = drive_location + f\"/data{data_size_to_extract}A.p\"\n",
    "data = pd.read_pickle(path_to_data)\n",
    "print(f\"Read time: {round(time.time() - t, 2)}s\")\n",
    "print(\"Total Sample Size:\" , data.shape[0])\n",
    "# data = data.head(8333)\n",
    "print(\"Current Sample Size:\" , data.shape[0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['img']\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "print(X[0].shape)\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    shuffle=True,\n",
    "    random_state=seed,\n",
    ")\n",
    "\n",
    "img = X[0]\n",
    " \n",
    "# scale down the image to one third\n",
    "img = rescale(img, 1/3, mode='reflect') \n",
    "\n",
    "# calculate the hog and return a visual representation.\n",
    "img_hog, img_hog_img = hog(img,\n",
    "                           pixels_per_cell=(12, 12),\n",
    "                           cells_per_block=(2,2),\n",
    "                           orientations=8,\n",
    "                           visualize=True,\n",
    "                           block_norm='L2-Hys')\n",
    " \n",
    "fig, ax = plt.subplots(1,2)\n",
    "fig.set_size_inches(16,12)\n",
    "# # remove ticks and their labels\n",
    "# [a.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "#     for a in ax]\n",
    " \n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('img')\n",
    "ax[1].imshow(img_hog_img, cmap='gray')\n",
    "ax[1].set_title('img hog')\n",
    "print('\\nNumber of pixels: ', img.shape[0] * img.shape[1])\n",
    "print('Number of hog features: ', img_hog.shape[0])\n",
    "\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.suptitle(\"Photo's per type\")\n",
    "plot_bar(y_train, loc='left')\n",
    "plot_bar(y_test, loc='right')\n",
    "plt.legend([\n",
    "    'train ({0} photos)'.format(len(y_train)),\n",
    "    'test ({0} photos)'.format(len(y_test))\n",
    "]);\n",
    "\n",
    "print(len(X_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HogTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Expects an array of 2d arrays (1 channel images)\n",
    "    Calculates hog features for each img\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, y=None, orientations=9,\n",
    "                 pixels_per_cell=(8, 8),\n",
    "                 cells_per_block=(3, 3), block_norm='L2-Hys'):\n",
    "        self.y = y\n",
    "        self.orientations = orientations\n",
    "        self.pixels_per_cell = pixels_per_cell\n",
    "        self.cells_per_block = cells_per_block\n",
    "        self.block_norm = block_norm\n",
    " \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    " \n",
    "    def transform(self, X, y=None):\n",
    " \n",
    "        def local_hog(X):\n",
    "            return hog(X,\n",
    "                       orientations=self.orientations,\n",
    "                       pixels_per_cell=self.pixels_per_cell,\n",
    "                       cells_per_block=self.cells_per_block,\n",
    "                       block_norm=self.block_norm)\n",
    " \n",
    "        try: # parallel\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "        except:\n",
    "            return np.array([local_hog(img) for img in X])\n",
    "\n",
    "\n",
    "hogify = HogTransformer(\n",
    "            pixels_per_cell=(15, 15),\n",
    "            cells_per_block=(3,3),\n",
    "            orientations=9,\n",
    "            block_norm='L2-Hys')\n",
    "\n",
    "scalify = StandardScaler()\n",
    "\n",
    "# X_train_hog = hogify.fit_transform(X_train)\n",
    "# X_train_processed = scalify.fit_transform(X_train_hog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD =  SGDClassifier(max_iter=1000, tol=1e-2, loss = \"hinge\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 100, verbose = 0)\n",
    "\n",
    "LR = LogisticRegression(random_state = seed, solver = 'saga', verbose = 0)\n",
    "\n",
    "SVCl = SVC(kernel='rbf', random_state = seed)\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state = seed)\n",
    "RF1 = RandomForestClassifier(n_estimators=1000, random_state = seed)\n",
    "RF2 = RandomForestClassifier(n_estimators=10, random_state = seed)\n",
    "RF3 = RandomForestClassifier(n_estimators=10000, random_state = seed)\n",
    "\n",
    "KNN = KNeighborsClassifier()\n",
    "\n",
    "SGD1og =  SGDClassifier(max_iter=1000, tol=1e-2, loss = \"log\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 100, verbose = 0)\n",
    "\n",
    "SGDmodified_huber =  SGDClassifier(max_iter=1000, tol=1e-2, loss = \"modified_huber\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 100, verbose = 0)\n",
    "\n",
    "SGDsquared_hinge =  SGDClassifier(max_iter=1000, tol=1e-2, loss = \"squared_hinge\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 100, verbose = 0)\n",
    "\n",
    "SGDperceptron =  SGDClassifier(max_iter=1000, tol=1e-2, loss = \"perceptron\", \n",
    "                     random_state = seed, warm_start = True, \n",
    "                     n_iter_no_change= 100, verbose = 0)\n",
    "\n",
    "MLP = MLPClassifier(random_state=seed, max_iter=200)\n",
    "\n",
    "# models = []\n",
    "# models.append((\"SGD\", SGD))\n",
    "# models.append(('LR', LR))\n",
    "# models.append(('NB', GaussianNB()))\n",
    "# models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "# models.append(('KNN', KNN))\n",
    "# models.append(('CART', DecisionTreeClassifier(random_state = seed)))\n",
    "# models.append(('RF', RF))\n",
    "# models.append(('SVM', SVCl))\n",
    "# models.append((\"MLP\", MLP))\n",
    "\n",
    "\n",
    "# # variables to hold the results and names\n",
    "# results0 = []\n",
    "# names   = []\n",
    "\n",
    "# # 10-fold cross validation\n",
    "# def ml_comparison(name, model, seed):\n",
    "#     scoring = \"accuracy\"\n",
    "#     t = time.time()\n",
    "#     kfold = KFold(n_splits=10, random_state = seed)\n",
    "#     cv_results = cross_val_score(model, X_train_processed, y_train, cv=kfold, scoring=scoring)\n",
    "#     run_time = round(time.time() - t)\n",
    "#     return cv_results, name, run_time, cv_results.mean()\n",
    "\n",
    "\n",
    "# jobs = 1\n",
    "\n",
    "# if num_cores//2 < len(models):\n",
    "#     jobs = num_cores//2\n",
    "# else:\n",
    "#     jobs = len(models)\n",
    "\n",
    "# print(f\"Using {jobs} cores\")\n",
    "    \n",
    "# current = Parallel(n_jobs = jobs, verbose=12)(delayed(ml_comparison)(name, model, seed) for name, model in models)\n",
    "\n",
    "# while current[0] == []:\n",
    "#     current.pop(0)\n",
    "\n",
    "# temp = list(zip(*current))\n",
    "# results0 = temp[0]\n",
    "# names = temp[1]\n",
    "# run_times = temp[2]    \n",
    "# accuracies = temp[3]    \n",
    "\n",
    "# for i in range(len(run_times)):\n",
    "#     left_aligned = f\"{names[i]}:\"\n",
    "#     center = f\"{round(accuracies[i], 5)}\"\n",
    "#     right_aligned = f\"({run_times[i]}s)\"\n",
    "#     print(f\"{left_aligned:<10}{center:^10}{right_aligned:>10}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotdata = []\n",
    "# plotdata.append((\"SGD\", 0.75203, 938))\n",
    "# plotdata.append((\"LR\", 0.75968, 654))\n",
    "# plotdata.append((\"NB\", 0.68227, 23))\n",
    "# plotdata.append((\"LDA\", 0.68017, 688))\n",
    "# plotdata.append((\"KNN\", 0.65467, 901))\n",
    "# plotdata.append((\"CART\", 0.67747, 742))\n",
    "# plotdata.append((\"RF\", 0.79823, 453))\n",
    "# plotdata.append((\"SVM\", 0.82613, 3481))\n",
    "# plotdata.append((\"MLP\", 0.80288, 404))\n",
    "# df = list(zip(*plotdata))\n",
    "# import matplotlib.cm as cm\n",
    "# plt.figure(figsize = (16,8))\n",
    "# plt.scatter(x=df[2], y=df[1], s = 100, color = plt.get_cmap('plasma')(np.linspace(0, 1, len(df[1]))))\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel(\"Log Runtime (s)\")\n",
    "# plt.ylabel(\"Accuracy (F1-Score)\")\n",
    "# plt.xlim(0,10500)\n",
    "# plt.ylim(0.64,0.86)\n",
    "\n",
    "\n",
    "# fontsize = 12\n",
    "# weight = 'bold'\n",
    "\n",
    "# i = 0\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(0,10), # distance from text to points (x,y)\n",
    "#              ha='center',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 1\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(0,10), # distance from text to points (x,y)\n",
    "#              ha='center',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 2\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(0,10), # distance from text to points (x,y)\n",
    "#              ha='center',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 3\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(0,10), # distance from text to points (x,y)\n",
    "#              ha='right',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 4\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(10,-10), # distance from text to points (x,y)\n",
    "#              ha='left',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 5\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(10,-10), # distance from text to points (x,y)\n",
    "#              ha='left',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 6\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(10,-10), # distance from text to points (x,y)\n",
    "#              ha='left',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 7\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(10,-20), # distance from text to points (x,y)\n",
    "#              ha='left',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "\n",
    "# i = 8\n",
    "# x = df[2][i]\n",
    "# y = df[1][i]\n",
    "# z = df[0][i]\n",
    "# label = f\"{z}\\n{x}s\"\n",
    "\n",
    "# # this method is called for each point\n",
    "# plt.annotate(label, # this is the text\n",
    "#              (x,y), # this is the point to label\n",
    "#              textcoords=\"offset points\", # how to position the text\n",
    "#              xytext=(0,10), # distance from text to points (x,y)\n",
    "#              ha='center',\n",
    "#              weight = weight, fontsize = fontsize)\n",
    "# plt.title(\"Accuracy vs Runtime of different ML Models\")\n",
    "# plt.savefig('fig12.pdf')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.patches as patches\n",
    "# custom_style = {'axes.labelcolor': 'white',\n",
    "#                 'xtick.color': 'white',\n",
    "#                 'ytick.color': 'white'}\n",
    "\n",
    "# # boxplot algorithm comparison\n",
    "# fig = plt.figure(figsize = (15,6))\n",
    "# fig.suptitle('ML Algorithm Comparison (1% Full Data)')\n",
    "# sns.set(style=\"ticks\", rc = custom_style, palette=\"pastel\")\n",
    "# ax = sns.boxplot(data = results0)\n",
    "# ax.set_xticklabels(names)\n",
    "# ax.patch.set_facecolor('black')\n",
    "# ax.set_ylabel(\"Accuracy\")\n",
    "# ax.set_xlabel(\"Model\")\n",
    "\n",
    "# from matplotlib.legend_handler import HandlerBase\n",
    "# from matplotlib.text import Text\n",
    "# from matplotlib.legend import Legend\n",
    "\n",
    "\n",
    "# class TextHandlerB(HandlerBase):\n",
    "#     def create_artists(self, legend, text ,xdescent, ydescent,\n",
    "#                         width, height, fontsize, trans):\n",
    "#         tx = Text(width/2.,height/2, text, fontsize=fontsize,\n",
    "#                   ha=\"center\", va=\"center\", fontweight=\"bold\", color = 'white')\n",
    "#         return [tx]\n",
    "\n",
    "# Legend.update_default_handler_map({str : TextHandlerB()})\n",
    "    \n",
    "    \n",
    "# handles = [\"SGD\", \"LR\", \"NB\", \"LDA\", \"KNN\", \"CART\", \"RF\", \"SVM\",\"MLP\"]\n",
    "# labels = [\"Stochastic Gradient Descent\", \"Logistic Regression\", \"Gaussian Naive Bayes\", \"Linear Discriminant Analysis\",\n",
    "#           \"K-Nearest Neighbours\", \"Decision Tree Classifier\", \"Random Forest Classifier\",\n",
    "#           \"Support Vector Machines\",\"Multi-Layer Perceptron\"]\n",
    "\n",
    "# l = ax.legend(handles=handles, labels=labels, framealpha = 0.1, borderpad = 0.8)\n",
    "\n",
    "# for text in l.get_texts():\n",
    "#     text.set_color(\"white\")\n",
    "\n",
    "# plt.savefig(\"fig8.pdf\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HOG_pipeline = Pipeline([\n",
    "    ('hogify', HogTransformer(\n",
    "        pixels_per_cell=(15, 15),\n",
    "        cells_per_block=(3,3),\n",
    "        orientations=20,\n",
    "        block_norm='L2-Hys')\n",
    "    ),\n",
    "    ('scalify', StandardScaler()),\n",
    "#     ('classify', SGD)\n",
    "#     ('classify', SVCl)\n",
    "    ('classify', MLP)\n",
    "#     ('classify', LR)\n",
    "])\n",
    " \n",
    "param_grid = [\n",
    "    {'hogify__orientations': [9,18],\n",
    "     'hogify__cells_per_block': [(3, 3), (2, 2)],\n",
    "     'hogify__pixels_per_cell': [(15, 15), (9,9), (18,18)],\n",
    "     'classify': [SGD, LR, RF, KNN]}\n",
    "]    \n",
    "\n",
    "grid_search = GridSearchCV(HOG_pipeline,\n",
    "                           param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs= 40,\n",
    "                           scoring='accuracy',\n",
    "                           verbose=10,\n",
    "                           return_train_score=True)\n",
    "t1 = time.time()\n",
    "# clf = grid_search.fit(X_train, y_train) # Usually 4% more than HOG_pipline direct\n",
    "clf = HOG_pipeline.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(f\"Total Runtime: {round(t2-t1, 2)}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# print(\"Best Config:\", clf.best_params_)\n",
    "# print(clf.best_estimator_)\n",
    "# print(f'Train Data Best Score: {clf.best_score_ * 100}')\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "index = X_test.index[0]\n",
    "cmx = confusion_matrix(y_test, y_pred)\n",
    "data_predictions = pd.DataFrame(data=y_pred, index= X_test.index)\n",
    "print('X_test Percentage correct: ', accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", cmx)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "group_names = ['True Positive',\n",
    "               'False Negative',\n",
    "               'False Positive',\n",
    "               'True Negative']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cmx.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cmx.flatten()/np.sum(cmx)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cmx, annot=labels, fmt='', cmap='Blues')\n",
    "plt.title(\"Percentage Confusion Matrix of Test Data\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.savefig(\"cmx1.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_iters  = min(len(X_test), 200)\n",
    "results = pd.DataFrame({'tile_id': data.tile_id, 'item_loc': data.item_loc, 'prediction': y_pred, \n",
    "                        'actual': y_test}, index = X_test.index)\n",
    "results = results.sort_values(by=['tile_id'])\n",
    "results = results.head(max_iters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "row = tiles_coord[tiles_coord.tile_id == results.iloc[0].tile_id].squeeze()\n",
    "irow = row.name\n",
    "jp = load_file(row.obsid)\n",
    "img = get_image(tiles_coord, jp, irow)\n",
    "ax.imshow(img)\n",
    "tile_id = results.loc[results.index[0]].tile_id\n",
    "\n",
    "for index, result in tqdm(results.iterrows(), total=results.shape[0]):\n",
    "    row = tiles_coord[tiles_coord.tile_id == result.tile_id].squeeze()\n",
    "    irow = row.name\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    \n",
    "    if not tile_id == result.tile_id:\n",
    "        fig, ax = plt.subplots(figsize=(12,12)) \n",
    "        ax.imshow(img)\n",
    "    \n",
    "    rect = Rectangle(result.item_loc[0], width = result.item_loc[1] , \n",
    "                     height = result.item_loc[2],\n",
    "                     fill=False, edgecolor='blue', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    fontsize = 12\n",
    "    h_offset = 0.3\n",
    "    label = f\"{result.actual}\"\n",
    "#     ax.text(result.item_loc[0][0], result.item_loc[0][1] - h_offset * fontsize, label, \n",
    "#             fontsize= fontsize, weight='bold', color = \"blue\")\n",
    "\n",
    "    label_ = f\"{result.prediction}\"\n",
    "    ax.text(result.item_loc[0][0] + result.item_loc[1] - 9 * len(label_), \n",
    "            result.item_loc[0][1] - h_offset * fontsize, label_, \n",
    "            fontsize= fontsize, weight='bold', color = \"darkred\")\n",
    "    \n",
    "    tile_id = result.tile_id\n",
    "    ax.set_title(f'{tile_id}')\n",
    "    plt.savefig(f\"{tile_id}.pdf\")\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def extract_image(tile_id, item_loc):\n",
    "#     row = tiles_coord[tiles_coord.tile_id == tile_id].squeeze()\n",
    "#     irow = row.name\n",
    "#     jp = load_file(row.obsid)\n",
    "#     img8 = get_image(tiles_coord, jp, irow)\n",
    "#     minx = item_loc[0][0]\n",
    "#     miny = item_loc[0][1]\n",
    "#     maxx = minx + item_loc[1]\n",
    "#     maxy = miny + item_loc[2]\n",
    "#     minx, miny, maxx, maxy = expand_bbox(minx, miny, maxx, maxy, scale = 1.2)\n",
    "#     processing0 = img8[miny:maxy, minx:maxx]\n",
    "#     processing1 = cv2.cvtColor(processing0, cv2.COLOR_BGR2GRAY)\n",
    "#     processing2 = resize(image = processing1, output_shape = rescale_size, anti_aliasing=True)\n",
    "#     processing3 = hog(processing2,\n",
    "#                     orientations= 9,\n",
    "#                     pixels_per_cell= (8, 8),\n",
    "#                     cells_per_block= (3, 3),\n",
    "#                     block_norm = 'L2-Hys')\n",
    "        \n",
    "        \n",
    "#     extracted_image = processing3\n",
    "#     row['img'] = extracted_image\n",
    "#     return extracted_image\n",
    "# # USE combined_classify_false_negative.json\n",
    "# # file_str = \"combined_classify_false_negative.json\"\n",
    "# # file_str = \"classify_false_negative.json\"\n",
    "# file_str = \"fan_classify_false_negative.json\"\n",
    "# with open(file_str, \"r\") as f:\n",
    "#     load = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "# predicted_fans = pd.DataFrame(load)#.sort_values(by=['tile_id'])\n",
    "# predicted_fans[\"isfan\"].replace({0: \"notfan\", 1: \"fan\"}, inplace=True)\n",
    "\n",
    "# # [(minx, miny), width, height]\n",
    "# item_loc = [(predicted_fans.bbox[0][0], predicted_fans.bbox[0][1]),\n",
    "#             predicted_fans.bbox[0][2],\n",
    "#             predicted_fans.bbox[0][3]]\n",
    "\n",
    "# predicted_fans.loc[:, 'item_loc'] = predicted_fans.bbox.map(lambda x: [(int(x[0]), int(x[1])),\n",
    "#                                                                         int(x[2]), int(x[3])])\n",
    "# predicted_fans = predicted_fans.drop(columns = ['bbox']).rename(columns={'isfan':'label'})\n",
    "\n",
    "# predicted_fans['img'] = 1\n",
    "\n",
    "# predicted_fans = predicted_fans.loc[~predicted_fans['tile_id'].str.contains(r'_{1}')]  \n",
    "# predicted_fans.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# print(f\"Tasks: {predicted_fans.shape[0]}\" )\n",
    "\n",
    "# extracted_list = Parallel(n_jobs = 50, verbose=6, prefer = 'threads')\\\n",
    "#                     (delayed(extract_image)(predicted_fans.iloc[i].tile_id,\n",
    "#                                             predicted_fans.iloc[i].item_loc) for i in range(predicted_fans.shape[0]))\n",
    "\n",
    "# predicted_fans['img'] = pd.Series(extracted_list)\n",
    "\n",
    "# t = time.time()\n",
    "# path_to_class = drive_location + \"/classify_false_negative_all_D.p\"\n",
    "# predicted_fans.to_pickle(path_to_class, protocol = 3)\n",
    "# print(f\"Write time: {round(time.time() - t, 2)}s\")\n",
    "\n",
    "# predicted_fans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "path_to_class = drive_location + \"/classify_false_negative_all_C.p\"\n",
    "predicted_fans = pd.read_pickle(path_to_class)\n",
    "print(f\"Read time: {round(time.time() - t, 2)}s\")\n",
    "predicted_fans = predicted_fans.head(1000)\n",
    "predicted_fans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "predictions = clf.predict(predicted_fans['img'])\n",
    "predicted_fans['prediction'] = predictions\n",
    "pred = predicted_fans['prediction']\n",
    "predicted_fans.drop(labels=['prediction'], axis=1,inplace = True)\n",
    "predicted_fans.insert(2, 'prediction', pred)\n",
    "y_test = predicted_fans['label']\n",
    "cmx = confusion_matrix(y_test, predictions)\n",
    "print(f\"Computing time: {round(time.time() - t, 2)}s\")\n",
    "print('Prediction Percentage Overlap: ', accuracy_score(predictions, y_test))\n",
    "print(\"Confusion Matrix:\\n\", cmx)\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "group_names = ['We all agree these are fan',\n",
    "               'Aslesha and ground truth say fan \\n I say not fan',\n",
    "               'Aslesha and I say fan \\n Ground truth say not fan',\n",
    "               'Ground truth and I say not fans \\n Aslesha says fans']\n",
    "# group_names = [\"ground truth says blotch\",\n",
    "#                \"ground truth says blotch\",\n",
    "#                \"ground truth says blotch\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says neither\",\n",
    "#                \"ground truth says fan\",\n",
    "#                \"ground truth says fan\",\n",
    "#                \"ground truth says fan\"]\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cmx.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cmx.flatten()/np.sum(cmx)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cmx, annot=labels, fmt='', cmap='Blues')\n",
    "plt.title(\"Percentage Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# max_iters  = min(len(predicted_fans['img']), 50)\n",
    "# results = pd.DataFrame({'tile_id': predicted_fans.tile_id, 'item_loc': predicted_fans.item_loc,\n",
    "#                         'actual': predicted_fans['label'], \n",
    "#                         'prediction': predictions}, index = predicted_fans['img'].index)\n",
    "\n",
    "# results = results.sort_values(by=['tile_id'])\n",
    "# results = results.tail(max_iters)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,12)) \n",
    "# row = tiles_coord[tiles_coord.tile_id == results.iloc[0].tile_id].squeeze()\n",
    "# irow = row.name\n",
    "# jp = load_file(row.obsid)\n",
    "# img = get_image(tiles_coord, jp, irow)\n",
    "# ax.imshow(img)\n",
    "# tile_id = results.loc[results.index[0]].tile_id\n",
    "\n",
    "# for index, result in tqdm(results.iterrows(), total=results.shape[0]):\n",
    "#     row = tiles_coord[tiles_coord.tile_id == result.tile_id].squeeze()\n",
    "#     irow = row.name\n",
    "#     jp = load_file(row.obsid)\n",
    "#     img = get_image(tiles_coord, jp, irow)\n",
    "    \n",
    "#     if not tile_id == result.tile_id:\n",
    "#         fig, ax = plt.subplots(figsize=(12,12)) \n",
    "#         ax.imshow(img)\n",
    "    \n",
    "#     rect = Rectangle(result.item_loc[0], width = result.item_loc[1] , \n",
    "#                      height = result.item_loc[2],\n",
    "#                      fill=False, edgecolor='blue', linewidth=1.5)\n",
    "#     ax.add_patch(rect)\n",
    "\n",
    "#     fontsize = 12\n",
    "#     h_offset = 0.32\n",
    "#     label = f\"{result.actual}\"\n",
    "#     label_ = f\"{result.prediction}\"\n",
    "#     ax.text(result.item_loc[0][0], \n",
    "#             result.item_loc[0][1] + result.item_loc[2] + 8 + h_offset * fontsize, label_, \n",
    "#             fontsize= fontsize, weight='bold', color = \"blue\")\n",
    "\n",
    "#     ax.text(result.item_loc[0][0], result.item_loc[0][1] - h_offset * fontsize, label, \n",
    "#             fontsize= fontsize, weight='bold', color = \"darkred\")\n",
    "    \n",
    "#     tile_id = result.tile_id\n",
    "#     ax.set_title(f'{tile_id}')\n",
    "#     red_patch = mpatches.Patch(color='darkred', label='Ground truth')\n",
    "#     blue_patch = mpatches.Patch(color='blue', label='Prediction')\n",
    "#     ax.legend(handles=[red_patch, blue_patch], fancybox=True, framealpha=0.5, shadow=True, borderpad=1)\n",
    "# plt.show()\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Only plotting when Aslesha and I agree on what is a fan but the ground truth says no. \n",
    "max_iters  = min(len(predicted_fans['img']), 20)\n",
    "results = pd.DataFrame({'tile_id': predicted_fans.tile_id, 'item_loc': predicted_fans.item_loc,\n",
    "                        'actual': predicted_fans['label'], \n",
    "                        'predictions': predictions}, index = predicted_fans['img'].index)\n",
    "\n",
    "results = results.sort_values(by=['tile_id'])\n",
    "results = results.tail(max_iters)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12)) \n",
    "row = tiles_coord[tiles_coord.tile_id == results.iloc[0].tile_id].squeeze()\n",
    "irow = row.name\n",
    "jp = load_file(row.obsid)\n",
    "img = get_image(tiles_coord, jp, irow)\n",
    "ax.imshow(img)\n",
    "tile_id = results.loc[results.index[0]].tile_id\n",
    "\n",
    "for index, result in tqdm(results.iterrows(), total=results.shape[0]):\n",
    "#     if result.actual == \"fan\" or result.predictions == \"notfan\":\n",
    "#         continue\n",
    "    \n",
    "    row = tiles_coord[tiles_coord.tile_id == result.tile_id].squeeze()\n",
    "    irow = row.name\n",
    "    jp = load_file(row.obsid)\n",
    "    img = get_image(tiles_coord, jp, irow)\n",
    "    \n",
    "    if not tile_id == result.tile_id:\n",
    "        fig, ax = plt.subplots(figsize=(12,12)) \n",
    "        ax.imshow(img)\n",
    "    \n",
    "    rect = Rectangle(result.item_loc[0], width = result.item_loc[1] , \n",
    "                     height = result.item_loc[2],\n",
    "                     fill=False, edgecolor='blue', linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    fontsize = 12\n",
    "    h_offset = 0.32\n",
    "    ax.text(result.item_loc[0][0], \n",
    "            result.item_loc[0][1] + result.item_loc[2] + 8 + h_offset * fontsize, f\"{result.predictions}\", \n",
    "            fontsize= fontsize, weight='bold', color = \"blue\")\n",
    "\n",
    "    ax.text(result.item_loc[0][0], result.item_loc[0][1] - h_offset * fontsize, f\"{result.actual}\", \n",
    "            fontsize= fontsize, weight='bold', color = \"darkred\")\n",
    "    \n",
    "    tile_id = result.tile_id\n",
    "    ax.set_title(f'{tile_id}')\n",
    "    red_patch = mpatches.Patch(color='darkred', label='Ground truth')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='Prediction')\n",
    "    ax.legend(handles=[red_patch, blue_patch], fancybox=True, framealpha=0.5, shadow=True, borderpad=1)\n",
    "plt.show()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab = predicted_fans['label']\n",
    "# predicted_fans.drop(labels=['label'], axis=1,inplace = True)\n",
    "# del predicted_fans['img']\n",
    "# predicted_fans.insert(2, 'ground_truth', lab)\n",
    "# t1 = time.time()\n",
    "# predicted_fans.to_json(\"processed_classify_false_negative.json\")\n",
    "# t2 = time.time()\n",
    "# print(f\"Write Time: {t2-t1}\")\n",
    "# predicted_fans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
